{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aab1a524",
   "metadata": {
    "_cell_guid": "9e403137-c828-4af0-ab32-0a2d27dee826",
    "_uuid": "8871452b-0eb2-40f7-8e37-deaadbd783bb",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-05-02T09:35:41.939054Z",
     "iopub.status.busy": "2025-05-02T09:35:41.938736Z",
     "iopub.status.idle": "2025-05-02T09:35:57.208041Z",
     "shell.execute_reply": "2025-05-02T09:35:57.206996Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 15.279497,
     "end_time": "2025-05-02T09:35:57.213898",
     "exception": false,
     "start_time": "2025-05-02T09:35:41.934401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import json\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Config:\n",
    "    TARGET = 'AWS'\n",
    "    USE_LAG_FEATURES = True\n",
    "    USE_ROLLING_STATISTICS = True\n",
    "    MIN_VALUE = 0.0  # Minimum allowed value\n",
    "    MAX_VALUE = 10.0  # Maximum allowed value\n",
    "    PLOT_MAX = 1   \n",
    "\n",
    "selected_features = [\n",
    "    'TCW', 'TCLW', 'R250', 'R500', 'R850', 'U850', 'V850', 'EWSS', 'KX', 'CAPE', 'SSHF', 'PEV'\n",
    "]\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.0, time_step_out=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, time_step_out)\n",
    "        self.time_step_out = time_step_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "def predict_with_model(model, test_x):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, test_x.shape[0], 128):  \n",
    "            batch_x = test_x[i:i+128].to(DEVICE)\n",
    "            batch_pred = model(batch_x).cpu().numpy()\n",
    "            batch_pred = np.clip(batch_pred, Config.MIN_VALUE, Config.MAX_VALUE)\n",
    "            predictions.append(batch_pred)\n",
    "    return np.vstack(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c953339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df, lag_steps=None, window_sizes=None):\n",
    "    result_df = df.copy()\n",
    "    if lag_steps:\n",
    "        for lag in lag_steps:\n",
    "            lag_col = f'{Config.TARGET}_lag{lag}'\n",
    "            if lag_col in result_df.columns:\n",
    "                result_df[lag_col] = result_df[lag_col].fillna(0)\n",
    "    if window_sizes:\n",
    "        for window in window_sizes:\n",
    "            mean_col = f'{Config.TARGET}_rollmean_{window}'\n",
    "            std_col = f'{Config.TARGET}_rollstd_{window}'\n",
    "            \n",
    "            if mean_col in result_df.columns:\n",
    "                result_df[mean_col] = result_df[mean_col].fillna(0)\n",
    "            \n",
    "            if std_col in result_df.columns:\n",
    "                result_df[std_col] = result_df[std_col].fillna(0)\n",
    "    result_df = result_df.fillna(0)\n",
    "    return result_df\n",
    "\n",
    "def prepare_data_for_prediction(train_df, val_df, test_df, best_params):\n",
    "    time_step_in = best_params[\"time_step_in\"]\n",
    "    time_step_out = best_params[\"time_step_out\"]\n",
    "    stride = best_params[\"stride\"]\n",
    "    \n",
    "    # Extract lag steps and window sizes\n",
    "    num_lags = best_params.get(\"num_lags\", 0)\n",
    "    lag_steps = [best_params[f\"lag_{i}\"] for i in range(num_lags)] if num_lags > 0 else []\n",
    "    \n",
    "    num_windows = best_params.get(\"num_windows\", 0)\n",
    "    window_sizes = [best_params[f\"window_{i}\"] for i in range(num_windows)] if num_windows > 0 else []\n",
    "    \n",
    "    # Make sure all dataframes are sorted by time\n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        if 'DATETIME' in df.columns:\n",
    "            df.sort_values(\"DATETIME\", inplace=True)\n",
    "    original_test_df = test_df.copy()\n",
    "    \n",
    "    # 1. Create lag features\n",
    "    if Config.USE_LAG_FEATURES and lag_steps:\n",
    "        combined_df = pd.concat([train_df, val_df, test_df]).reset_index(drop=True)\n",
    "        combined_df = create_lag_features(combined_df, Config.TARGET, lag_steps)\n",
    "        test_df = combined_df.iloc[len(train_df) + len(val_df):].reset_index(drop=True)\n",
    "    \n",
    "    # 2. Create rolling statistics\n",
    "    if Config.USE_ROLLING_STATISTICS and window_sizes:\n",
    "        combined_df = pd.concat([train_df, val_df, test_df]).reset_index(drop=True)\n",
    "        combined_df = create_rolling_statistics(combined_df, Config.TARGET, window_sizes)\n",
    "        test_df = combined_df.iloc[len(train_df) + len(val_df):].reset_index(drop=True)\n",
    "    \n",
    "    # 3. Handle missing values\n",
    "    test_df = handle_missing_values(test_df, lag_steps, window_sizes)\n",
    "    \n",
    "    # 4. Prepare feature columns\n",
    "    basic_cols = [col for col in selected_features if col in test_df.columns]\n",
    "    lag_cols = [f'{Config.TARGET}_lag{lag}' for lag in lag_steps if f'{Config.TARGET}_lag{lag}' in test_df.columns]\n",
    "    roll_cols = []\n",
    "    \n",
    "    for window in window_sizes:\n",
    "        mean_col = f'{Config.TARGET}_rollmean_{window}'\n",
    "        std_col = f'{Config.TARGET}_rollstd_{window}'\n",
    "        if mean_col in test_df.columns:\n",
    "            roll_cols.append(mean_col)\n",
    "        if std_col in test_df.columns:\n",
    "            roll_cols.append(std_col)\n",
    "    \n",
    "    feature_cols = basic_cols + lag_cols + roll_cols\n",
    "    # 5. Create sequences\n",
    "    test_x, test_y = create_sequences(test_df, feature_cols, Config.TARGET, time_step_in, time_step_out, stride)\n",
    "    return test_x, original_test_df, feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ccb4edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(df, input_cols, target_col, time_step_in, time_step_out=1, stride=1):\n",
    "    sequences, targets = [], []\n",
    "    grouped = df.groupby(['ROW', 'COL'])\n",
    "    \n",
    "    for _, group in grouped:\n",
    "        if 'DATETIME' in group.columns:\n",
    "            group = group.sort_values(\"DATETIME\")\n",
    "        data = group[input_cols].values\n",
    "        target_data = group[target_col].values\n",
    "        \n",
    "        if len(data) < time_step_in + time_step_out:\n",
    "            continue\n",
    "        \n",
    "        for i in range(0, len(data) - time_step_in - time_step_out + 1, stride):\n",
    "            seq = data[i:i+time_step_in]\n",
    "            if time_step_out == 1:\n",
    "                target = target_data[i+time_step_in]\n",
    "                targets.append(target)\n",
    "            else:\n",
    "                target = target_data[i+time_step_in:i+time_step_in+time_step_out]\n",
    "                targets.append(target)\n",
    "            sequences.append(seq)\n",
    "    \n",
    "    if not sequences:\n",
    "        return None, None\n",
    "    \n",
    "    if time_step_out == 1:\n",
    "        return torch.tensor(sequences, dtype=torch.float32), torch.tensor(targets, dtype=torch.float32).unsqueeze(1)\n",
    "    else:\n",
    "        return torch.tensor(sequences, dtype=torch.float32), torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "def create_lag_features(df, target_column, lag_steps, groupby_cols=['ROW', 'COL']):\n",
    "    result_df = df.copy()\n",
    "    for lag in lag_steps:\n",
    "        result_df[f'{target_column}_lag{lag}'] = result_df.groupby(groupby_cols)[target_column].shift(lag)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def create_rolling_statistics(df, target_column, window_sizes, groupby_cols=['ROW', 'COL']):\n",
    "    result_df = df.copy()\n",
    "    for window in window_sizes:\n",
    "        result_df[f'{target_column}_rollmean_{window}'] = result_df.groupby(groupby_cols)[target_column].transform(\n",
    "            lambda x: x.rolling(window, min_periods=1).mean())\n",
    "        result_df[f'{target_column}_rollstd_{window}'] = result_df.groupby(groupby_cols)[target_column].transform(\n",
    "            lambda x: x.rolling(window, min_periods=1).std())\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f08444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_rainfall(test_df, predictions, time_step_in, sample_indices=None, num_samples=3, \n",
    "                    select_highest_rainfall=False, select_moderate_rainfall=False):\n",
    "    test_df = test_df.copy()\n",
    "    test_df['AWS'].replace([-np.inf, np.inf], np.nan, inplace=True)\n",
    "    mask = ~test_df['AWS'].isna()\n",
    "    test_df.loc[mask, 'AWS'] = np.clip(test_df.loc[mask, 'AWS'], Config.MIN_VALUE, Config.MAX_VALUE)\n",
    "    has_datetime = 'DATETIME' in test_df.columns\n",
    "    if has_datetime:\n",
    "        unique_timestamps = test_df['DATETIME'].unique()\n",
    "        valid_timestamps = unique_timestamps[time_step_in:]\n",
    "\n",
    "        if select_highest_rainfall or select_moderate_rainfall:\n",
    "            rainfall_by_timestamp = {}\n",
    "            for timestamp in valid_timestamps:\n",
    "                timestamp_data = test_df[test_df['DATETIME'] == timestamp].copy()\n",
    "                timestamp_data['AWS'].replace([-np.inf, np.inf], np.nan, inplace=True)\n",
    "                if timestamp_data['AWS'].isna().all():\n",
    "                    print(f\"Skipping timestamp {timestamp} because all AWS values are NaN or -inf\")\n",
    "                    continue\n",
    "                avg_rainfall = timestamp_data['AWS'].replace([-np.inf, np.inf], np.nan).dropna().mean()\n",
    "                if not np.isnan(avg_rainfall) and avg_rainfall != -np.inf:\n",
    "                    rainfall_by_timestamp[timestamp] = avg_rainfall\n",
    "            if not rainfall_by_timestamp:\n",
    "                if len(valid_timestamps) <= num_samples:\n",
    "                    sample_indices = list(range(len(valid_timestamps)))\n",
    "                else:\n",
    "                    sample_indices = np.random.choice(range(len(valid_timestamps)), num_samples, replace=False)\n",
    "                    sample_indices.sort()\n",
    "            else:\n",
    "                if select_highest_rainfall:\n",
    "                    sorted_timestamps = sorted(rainfall_by_timestamp.items(), key=lambda x: x[1], reverse=True)\n",
    "                    label = \"highest\"\n",
    "                elif select_moderate_rainfall:\n",
    "                    median_rainfall = np.median(list(rainfall_by_timestamp.values()))\n",
    "                    sorted_timestamps = sorted(rainfall_by_timestamp.items(), \n",
    "                                              key=lambda x: abs(x[1] - median_rainfall))\n",
    "                    label = \"moderate\"\n",
    "                selected_timestamps = [ts for ts, _ in sorted_timestamps[:num_samples]]\n",
    "\n",
    "                for i, (ts, avg_rain) in enumerate(sorted_timestamps[:10]):  # Show top 10 for reference\n",
    "                    if select_moderate_rainfall:\n",
    "                        median_rainfall = np.median(list(rainfall_by_timestamp.values()))\n",
    "                        diff_from_median = abs(avg_rain - median_rainfall)\n",
    "                        print(f\"Timestamp: {ts}, Average rainfall: {avg_rain:.4f}, Diff from median: {diff_from_median:.4f}\")\n",
    "                    else:\n",
    "                        print(f\"Timestamp: {ts}, Average rainfall: {avg_rain:.4f}\")\n",
    "                for ts in selected_timestamps:\n",
    "                    if select_moderate_rainfall:\n",
    "                        median_rainfall = np.median(list(rainfall_by_timestamp.values()))\n",
    "                        diff_from_median = abs(rainfall_by_timestamp[ts] - median_rainfall)\n",
    "                        print(f\"Timestamp: {ts}, Average rainfall: {rainfall_by_timestamp[ts]:.4f}, Diff from median: {diff_from_median:.4f}\")\n",
    "                    else:\n",
    "                        print(f\"Timestamp: {ts}, Average rainfall: {rainfall_by_timestamp[ts]:.4f}\")\n",
    "\n",
    "                sample_indices = []\n",
    "                for ts in selected_timestamps:\n",
    "                    try:\n",
    "                        idx = list(valid_timestamps).index(ts)\n",
    "                        sample_indices.append(idx)\n",
    "                    except ValueError:\n",
    "                        print(f\"Warning: Timestamp {ts} not found in valid_timestamps\")\n",
    "            \n",
    "            print(f\"Sample indices for visualization: {sample_indices}\")\n",
    "        elif sample_indices is None:\n",
    "            if len(valid_timestamps) <= num_samples:\n",
    "                sample_indices = list(range(len(valid_timestamps)))\n",
    "            else:\n",
    "                sample_indices = np.random.choice(range(len(valid_timestamps)), num_samples, replace=False)\n",
    "                sample_indices.sort()  # Sort for consistent ordering\n",
    "        \n",
    "        timestamps_to_plot = [valid_timestamps[i] for i in sample_indices]\n",
    "        for ts in timestamps_to_plot:\n",
    "            ts_data = test_df[test_df['DATETIME'] == ts]\n",
    "            print(f\"Timestamp: {ts}\")\n",
    "            print(f\"  - Mean AWS: {ts_data['AWS'].mean():.4f}\")\n",
    "            print(f\"  - Max AWS: {ts_data['AWS'].max():.4f}\")\n",
    "            print(f\"  - Min AWS: {ts_data['AWS'].min():.4f}\")\n",
    "            print(f\"  - Number of data points: {len(ts_data)}\")\n",
    "    else:\n",
    "        print(\"❌ No DATETIME column found in test_df. Cannot create visualization.\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(len(timestamps_to_plot), 2, figsize=(15, 5*len(timestamps_to_plot)))\n",
    "    if len(timestamps_to_plot) == 1:\n",
    "        axes = np.array([axes])  \n",
    "    colors_actual = [(1, 1, 1, 0.7), (0.8, 0.8, 1, 0.8), (0, 0, 1, 1)]  # White to blue with alpha\n",
    "    colors_pred = [(1, 1, 1, 0.7), (0.8, 0.8, 1, 0.8), (0, 0, 1, 1)]    # White to blue with alpha\n",
    "    \n",
    "    cmap_actual = LinearSegmentedColormap.from_list('actual_rainfall', colors_actual)\n",
    "    cmap_pred = LinearSegmentedColormap.from_list('predicted_rainfall', colors_pred)\n",
    "    \n",
    "    for i, timestamp in enumerate(timestamps_to_plot):\n",
    "        timestamp_data = test_df[test_df['DATETIME'] == timestamp].copy()\n",
    "        timestamp_indices = timestamp_data.index\n",
    "        timestamp_data['PREDICTED_AWS'] = 0\n",
    "        pred_start_idx = 0\n",
    "        for ts_idx in timestamp_indices:\n",
    "            if pred_start_idx < len(predictions):\n",
    "                if predictions[pred_start_idx].ndim > 0:\n",
    "                    pred_value = float(predictions[pred_start_idx].flatten()[0])\n",
    "                else:\n",
    "                    pred_value = float(predictions[pred_start_idx])\n",
    "                \n",
    "                pred_value = np.clip(pred_value, Config.MIN_VALUE, Config.MAX_VALUE)\n",
    "                \n",
    "                timestamp_data.loc[ts_idx, 'PREDICTED_AWS'] = pred_value\n",
    "                pred_start_idx += 1\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        # 1. Replace -inf/inf with NaN\n",
    "        timestamp_data['AWS'].replace([-np.inf, np.inf], np.nan, inplace=True)\n",
    "        \n",
    "        # 2. For the purpose of visualization, replace NaN with 0\n",
    "        # But first report how many values were NaN\n",
    "        nan_count = timestamp_data['AWS'].isna().sum()\n",
    "        if nan_count > 0:\n",
    "            print(f\"  - Replacing {nan_count} NaN values with 0 for timestamp {timestamp}\")\n",
    "        \n",
    "        # 3. Clip values to valid range\n",
    "        mask = ~timestamp_data['AWS'].isna()\n",
    "        timestamp_data.loc[mask, 'AWS'] = np.clip(timestamp_data.loc[mask, 'AWS'], Config.MIN_VALUE, Config.MAX_VALUE)\n",
    "        \n",
    "        # 4. Finally fill NaNs with 0 for visualization\n",
    "        timestamp_data['AWS'].fillna(0, inplace=True)\n",
    "        timestamp_data['PREDICTED_AWS'].fillna(0, inplace=True)\n",
    "        \n",
    "        # Set fixed color scaling range from 0 to 0.5\n",
    "        # Values above 0.5 will all appear as the maximum color intensity\n",
    "        vmin = 0.0\n",
    "        vmax = 0.5  # Fixed maximum for visualization purposes\n",
    "        \n",
    "        # Calculate metrics for this timestamp, excluding positions where AWS was NaN in original data\n",
    "        try:\n",
    "            valid_positions = ~test_df[test_df['DATETIME'] == timestamp]['AWS'].isna()\n",
    "            \n",
    "            if valid_positions.sum() > 0:\n",
    "                valid_actual = timestamp_data.loc[valid_positions.values, 'AWS']\n",
    "                valid_pred = timestamp_data.loc[valid_positions.values, 'PREDICTED_AWS']\n",
    "                \n",
    "                mse = mean_squared_error(valid_actual, valid_pred)\n",
    "                if len(set(valid_actual)) > 1:  # R² requires variance in the data\n",
    "                    r2 = r2_score(valid_actual, valid_pred)\n",
    "                else:\n",
    "                    r2 = np.nan\n",
    "                    print(f\"  - Cannot calculate R² for timestamp {timestamp}: No variance in actual values\")\n",
    "            else:\n",
    "                mse = np.nan\n",
    "                r2 = np.nan\n",
    "                print(f\"  - Cannot calculate metrics for timestamp {timestamp}: No valid AWS values\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating metrics for timestamp {timestamp}: {e}\")\n",
    "            mse = np.nan\n",
    "            r2 = np.nan\n",
    "\n",
    "        avg_actual = test_df[test_df['DATETIME'] == timestamp]['AWS'].replace([-np.inf, np.inf], np.nan).dropna().mean()\n",
    "        if np.isnan(avg_actual):\n",
    "            avg_actual = 0\n",
    "            \n",
    "        avg_pred = timestamp_data['PREDICTED_AWS'].mean()\n",
    "\n",
    "        ax_actual = axes[i, 0]\n",
    "        scatter_actual = ax_actual.scatter(\n",
    "            timestamp_data['COL'], timestamp_data['ROW'], \n",
    "            c=timestamp_data['AWS'], \n",
    "            cmap=cmap_actual,\n",
    "            s=30, alpha=0.9,  # Larger dots with more opacity\n",
    "            vmin=vmin, vmax=vmax,\n",
    "            edgecolors='none'\n",
    "        )\n",
    "        \n",
    "        ax_actual.set_title(f\"Actual Rainfall ({timestamp})\\nAvg: {avg_actual:.4f}\")\n",
    "        ax_actual.set_xlabel(\"Column\")\n",
    "        ax_actual.set_ylabel(\"Row\")\n",
    "        ax_actual.invert_yaxis()  # Invert y-axis to match image convention\n",
    "        ax_actual.grid(True, linestyle='--', alpha=0.7)  # Add grid for better visibility\n",
    "        ax_pred = axes[i, 1]\n",
    "        scatter_pred = ax_pred.scatter(\n",
    "            timestamp_data['COL'], timestamp_data['ROW'], \n",
    "            c=timestamp_data['PREDICTED_AWS'], \n",
    "            cmap=cmap_pred,\n",
    "            s=30, alpha=0.9,  # Larger dots with more opacity\n",
    "            vmin=vmin, vmax=vmax,\n",
    "            edgecolors='none'\n",
    "        )\n",
    "        \n",
    "        ax_pred.set_title(f\"Predicted Rainfall ({timestamp})\\nAvg: {avg_pred:.4f}, MSE: {mse:.4f}, R²: {r2:.4f}\")\n",
    "        ax_pred.set_xlabel(\"Column\")\n",
    "        ax_pred.set_ylabel(\"Row\")\n",
    "        ax_pred.invert_yaxis()  # Invert y-axis to match image convention\n",
    "        ax_pred.grid(True, linestyle='--', alpha=0.7)  # Add grid for better visibility\n",
    "        cbar_actual = plt.colorbar(scatter_actual, ax=ax_actual, label='Rainfall Intensity')\n",
    "        cbar_actual.set_label('Actual Rainfall (0 - 0.5+)', fontsize=10)\n",
    "        \n",
    "        cbar_pred = plt.colorbar(scatter_pred, ax=ax_pred, label='Rainfall Intensity')\n",
    "        cbar_pred.set_label('Predicted Rainfall (0 - 0.5+)', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('rainfall_prediction_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edca42ef",
   "metadata": {
    "papermill": {
     "duration": 0.009564,
     "end_time": "2025-05-02T09:35:57.233667",
     "exception": false,
     "start_time": "2025-05-02T09:35:57.224103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_path = \"/kaggle/input/ai-dataimputedataset-k-fold\"\n",
    "month = \"2020-04\"  # Update as needed: \"2019-04\", \"2019-10\", \"2020-04\", \"2020-10\"\n",
    "fold = \"fold_5\"    # Update as needed: \"fold_1\" to \"fold_5\"\n",
    "\n",
    "folder = os.path.join(base_path, month, fold)\n",
    "train_file = os.path.join(folder, \"processed_train.csv\")\n",
    "val_file = os.path.join(folder, \"processed_val.csv\")\n",
    "test_file = os.path.join(folder, \"processed_val.csv\")\n",
    "    \n",
    "best_params_path = \"/kaggle/input/lstm-checkpoint/best_params_{}.json\".format(month)\n",
    "checkpoint_path = \"/kaggle/input/lstm-checkpoint/best_model_{}.pt\".format(month)\n",
    "    \n",
    "train_df = load_data(train_file)\n",
    "val_df = load_data(val_file)\n",
    "test_df = load_data(test_file)\n",
    "\n",
    "with open(best_params_path, \"r\") as f:\n",
    "    best_params = json.load(f)\n",
    "    \n",
    "test_x, original_test_df, feature_cols = prepare_data_for_prediction(train_df, val_df, test_df, best_params)\n",
    "    \n",
    "input_size = test_x.shape[2]  # Number of features\n",
    "model = LSTMModel(\n",
    "    input_size=input_size,\n",
    "    hidden_size=best_params[\"hidden_size\"],\n",
    "    num_layers=best_params[\"num_layers\"],\n",
    "    dropout=best_params[\"dropout\"],\n",
    "    time_step_out=best_params[\"time_step_out\"]\n",
    ").to(DEVICE)\n",
    "    \n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=DEVICE))\n",
    "predictions = predict_with_model(model, test_x)\n",
    "visualize_rainfall(original_test_df, predictions, best_params[\"time_step_in\"], \n",
    "                      num_samples=3, select_moderate_rainfall=True)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7091976,
     "sourceId": 11340032,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7117290,
     "sourceId": 11369580,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7142988,
     "sourceId": 11403913,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6984394,
     "sourceId": 11443624,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7173004,
     "sourceId": 11448922,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7186553,
     "sourceId": 11467955,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19.809986,
   "end_time": "2025-05-02T09:35:58.865110",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-02T09:35:39.055124",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
