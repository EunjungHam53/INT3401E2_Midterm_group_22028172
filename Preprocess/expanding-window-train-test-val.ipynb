{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec430c0b",
   "metadata": {
    "papermill": {
     "duration": 0.002221,
     "end_time": "2025-03-27T15:51:08.624324",
     "exception": false,
     "start_time": "2025-03-27T15:51:08.622103",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# I; Split csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67dad2d7",
   "metadata": {
    "_cell_guid": "61119946-2f42-4e7d-a33b-aae7594994de",
    "_uuid": "e69a16f1-a59b-4cab-937c-a40d0cc575de",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-27T15:51:08.629839Z",
     "iopub.status.busy": "2025-03-27T15:51:08.629252Z",
     "iopub.status.idle": "2025-03-27T15:53:17.754871Z",
     "shell.execute_reply": "2025-03-27T15:53:17.753459Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 129.131844,
     "end_time": "2025-03-27T15:53:17.758279",
     "exception": false,
     "start_time": "2025-03-27T15:51:08.626435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing merged dataset for 2019-04: 334 points found.\n",
      "Fold 1: train 32064 rows, val 32064 rows, test 48096 rows\n",
      "Fold 2: train 64128 rows, val 32064 rows, test 48096 rows\n",
      "Fold 3: train 96192 rows, val 32064 rows, test 48096 rows\n",
      "Fold 4: train 128256 rows, val 32064 rows, test 48096 rows\n",
      "Fold 5: train 160320 rows, val 32064 rows, test 48096 rows\n",
      "Processing merged dataset for 2019-10: 345 points found.\n",
      "Fold 1: train 33120 rows, val 33120 rows, test 49680 rows\n",
      "Fold 2: train 66240 rows, val 33120 rows, test 49680 rows\n",
      "Fold 3: train 99360 rows, val 33120 rows, test 49680 rows\n",
      "Fold 4: train 132480 rows, val 33120 rows, test 49680 rows\n",
      "Fold 5: train 165600 rows, val 33120 rows, test 49680 rows\n",
      "Processing merged dataset for 2020-04: 334 points found.\n",
      "Fold 1: train 32064 rows, val 32064 rows, test 48096 rows\n",
      "Fold 2: train 64128 rows, val 32064 rows, test 48096 rows\n",
      "Fold 3: train 96192 rows, val 32064 rows, test 48096 rows\n",
      "Fold 4: train 128256 rows, val 32064 rows, test 48096 rows\n",
      "Fold 5: train 160320 rows, val 32064 rows, test 48096 rows\n",
      "Processing merged dataset for 2020-10: 345 points found.\n",
      "Fold 1: train 33120 rows, val 33120 rows, test 49680 rows\n",
      "Fold 2: train 66240 rows, val 33120 rows, test 49680 rows\n",
      "Fold 3: train 99360 rows, val 33120 rows, test 49680 rows\n",
      "Fold 4: train 132480 rows, val 33120 rows, test 49680 rows\n",
      "Fold 5: train 165600 rows, val 33120 rows, test 49680 rows\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "csv_path = {\"merged\": \"/kaggle/input/rain-precipitation-station/filter_merged_data\"}\n",
    "time_range = [\"2019-04\", \"2019-10\", \"2020-04\", \"2020-10\"]\n",
    "n_splits = 5\n",
    "timestamps_total = 24*30 \n",
    "timestamps_test = 24*6    \n",
    "timestamps_trainval = timestamps_total - timestamps_test \n",
    "output_path = \"/kaggle/working/\"\n",
    "\n",
    "for time in time_range:\n",
    "    cur_time_path = os.path.join(output_path, time)\n",
    "    if not os.path.exists(cur_time_path):\n",
    "        os.makedirs(cur_time_path)\n",
    "    \n",
    "    for key, path in csv_path.items():\n",
    "        file_name = f\"{key}_{time}.csv\"\n",
    "        path_name = os.path.join(path, file_name)\n",
    "        all_data = pd.read_csv(path_name)\n",
    "        # each point has 24*30 timestamps\n",
    "        n_points = all_data.shape[0] // timestamps_total\n",
    "        print(f\"Processing {key} dataset for {time}: {n_points} points found.\")\n",
    "        \n",
    "        # luu lai du lieu cac fold => 5 fold x 334 diem\n",
    "        train_folds = [[] for _ in range(n_splits)]\n",
    "        val_folds = [[] for _ in range(n_splits)]\n",
    "        test_folds = [[] for _ in range(n_splits)]\n",
    "        \n",
    "        # TimeSeriesSplit trên tập train_val với 576 timestamps.\n",
    "        # mọi điểm có số lượng train_val như nhau nên tính một lần và dùng cho tất cả.\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        folds_indices = list(tscv.split(np.arange(timestamps_trainval))) #tuple (train_idx, val_idx)\n",
    "        \n",
    "        for point in range(n_points):\n",
    "            point_data = all_data.iloc[point * timestamps_total: (point + 1) * timestamps_total, :].reset_index(drop=True)\n",
    "            train_val_data = point_data.iloc[:timestamps_trainval, :].reset_index(drop=True)\n",
    "            test_data = point_data.iloc[timestamps_trainval:, :].reset_index(drop=True)\n",
    "            \n",
    "            for i, (train_idx, val_idx) in enumerate(folds_indices):\n",
    "                train_fold = train_val_data.iloc[train_idx, :].copy()\n",
    "                val_fold = train_val_data.iloc[val_idx, :].copy()\n",
    "                \n",
    "                train_folds[i].append(train_fold)\n",
    "                val_folds[i].append(val_fold)\n",
    "                test_folds[i].append(test_data)\n",
    "        # to_csv\n",
    "        for i in range(n_splits):\n",
    "            fold_train = pd.concat(train_folds[i], axis=0).reset_index(drop=True)\n",
    "            fold_val = pd.concat(val_folds[i], axis=0).reset_index(drop=True)\n",
    "            fold_test = pd.concat(test_folds[i], axis=0).reset_index(drop=True)\n",
    "            \n",
    "            fold_folder = os.path.join(cur_time_path, f\"fold_{i+1}\")\n",
    "            if not os.path.exists(fold_folder):\n",
    "                os.makedirs(fold_folder)\n",
    "            \n",
    "            train_file = os.path.join(fold_folder, f\"{key}_train.csv\")\n",
    "            val_file = os.path.join(fold_folder, f\"{key}_val.csv\")\n",
    "            test_file = os.path.join(fold_folder, f\"{key}_test.csv\")\n",
    "            \n",
    "            fold_train.to_csv(train_file, index=False)\n",
    "            fold_val.to_csv(val_file, index=False)\n",
    "            fold_test.to_csv(test_file, index=False)\n",
    "            \n",
    "            print(f\"Fold {i+1}: train {fold_train.shape[0]} rows, val {fold_val.shape[0]} rows, test {fold_test.shape[0]} rows\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6898796,
     "sourceId": 11070607,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6904873,
     "sourceId": 11078483,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 133.595643,
   "end_time": "2025-03-27T15:53:18.584411",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-27T15:51:04.988768",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
